{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, context):\n",
    "    xs= []\n",
    "    ys = []\n",
    "    for word in words:\n",
    "\n",
    "        new_word =  word + '.'\n",
    "\n",
    "        new_added = ['.'] * (context)\n",
    "\n",
    "        contextt = \"\".join(new_added)\n",
    "        new_word = contextt + new_word\n",
    "\n",
    "        for i in range(len(new_word)):\n",
    "\n",
    "            slided_word = new_word[i: (i +context)%len(new_word)]\n",
    "            if not (i+context) == len(new_word):\n",
    "                xs.append([stoi[x] for x in list(slided_word)])\n",
    "                ys.append(stoi[new_word[(i+context)%len(new_word)]])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_forward_pass(X_dataset, Y_dataset, feature_vector_C, H_weights, H_bias, G_weights, G_bias):\n",
    "    for i in range(1):\n",
    " \n",
    "        minibatch_ints = torch.randint(0, X_dataset.shape[0], (32,))\n",
    "        minibatch_construct = X_dataset[minibatch_ints]\n",
    "\n",
    "        feature_activation_layer = feature_vector_C[minibatch_construct] \n",
    "        merged_feature_vector = feature_activation_layer.view((minibatch_construct.shape[0], -1)) \n",
    "        first_activation = torch.tanh(merged_feature_vector @ H_weights + H_bias)\n",
    "        second_activation = first_activation @ G_weights + G_bias\n",
    "        \n",
    "\n",
    "        normalized_probabilities = F.log_softmax(second_activation, dim=1)\n",
    "        negative_likelihood_loss = F.nll_loss(normalized_probabilities, Y_dataset[minibatch_ints])\n",
    "\n",
    "        return negative_likelihood_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.31351089477539\n",
      "2.847027063369751\n",
      "2.814969778060913\n",
      "2.946777582168579\n",
      "2.980334758758545\n",
      "2.906338930130005\n",
      "2.7683167457580566\n",
      "3.0382931232452393\n",
      "2.920518159866333\n",
      "2.725039005279541\n",
      "2.197007417678833\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "context=3\n",
    "Xtr, Ytr = build_dataset(words[:n1],context)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2],context)\n",
    "Xte, Yte = build_dataset(words[n2:],context)\n",
    "\n",
    "\n",
    "# META_VARIABLES\n",
    "H_weights_length = 200\n",
    "Context_length = 3\n",
    "Feature_dimensions = 10\n",
    "\n",
    "\n",
    "# Parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "feature_vector_C = torch.rand((27, Feature_dimensions), generator=g)\n",
    "H_weights = torch.rand((Context_length*Feature_dimensions, H_weights_length),generator=g)\n",
    "H_bias = torch.rand((H_weights_length) ,generator=g)\n",
    "G_weights = torch.rand((H_weights_length, 27) ,generator=g)\n",
    "G_bias = torch.rand((27,), generator=g)\n",
    "\n",
    "parameters = [feature_vector_C, H_weights, H_bias, G_weights, G_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "lowest_loss = 0\n",
    "for i in range(100000):\n",
    "# minibatch contruct\n",
    "    minibatch_ints = torch.randint(0, Xtr.shape[0], (32,))\n",
    "    minibatch_construct = Xtr[minibatch_ints]\n",
    "# forward pass\n",
    "    feature_activation_layer = feature_vector_C[minibatch_construct] \n",
    "    merged_feature_vector = feature_activation_layer.view((minibatch_construct.shape[0], -1)) \n",
    "    first_activation = torch.tanh(merged_feature_vector @ H_weights + H_bias)\n",
    "    second_activation = first_activation @ G_weights + G_bias\n",
    "    \n",
    "# loss\n",
    "    normalized_probabilities = F.log_softmax(second_activation, dim=1)\n",
    "    negative_loss_likelihood = F.nll_loss(normalized_probabilities, Ytr[minibatch_ints])\n",
    "# backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    negative_loss_likelihood.backward()\n",
    "\n",
    "#gradient descent\n",
    "\n",
    "    lr = 0.01 if i < 100000 else 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    \n",
    "    loss_item = negative_loss_likelihood.item()\n",
    "    if i == 1:\n",
    "        lowest_loss = loss_item\n",
    "    if lowest_loss > loss_item:\n",
    "        lowest_loss = loss_item\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(loss_item)\n",
    "\n",
    "\n",
    "print(lowest_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2812) testing loss on validation set\n"
     ]
    }
   ],
   "source": [
    "testing = test_forward_pass(Xdev, Ydev, feature_vector_C, H_weights, H_bias, G_weights, G_bias)\n",
    "print(testing, 'testing loss on validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
