{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, context):\n",
    "    xs= []\n",
    "    ys = []\n",
    "    for word in words:\n",
    "\n",
    "        new_word =  word + '.'\n",
    "\n",
    "        new_added = ['.'] * (context)\n",
    "\n",
    "        contextt = \"\".join(new_added)\n",
    "        new_word = contextt + new_word\n",
    "\n",
    "        for i in range(len(new_word)):\n",
    "\n",
    "            slided_word = new_word[i: (i +context)%len(new_word)]\n",
    "            if not (i+context) == len(new_word):\n",
    "                xs.append([stoi[x] for x in list(slided_word)])\n",
    "                ys.append(stoi[new_word[(i+context)%len(new_word)]])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_forward_pass(X_dataset, Y_dataset, feature_vector_C, H_weights, H_bias, G_weights, G_bias):\n",
    "    for i in range(1):\n",
    " \n",
    "        minibatch_ints = torch.randint(0, X_dataset.shape[0], (32,))\n",
    "        minibatch_construct = X_dataset[minibatch_ints]\n",
    "\n",
    "        feature_activation_layer = feature_vector_C[X_dataset] \n",
    "        merged_feature_vector = feature_activation_layer.view((X_dataset.shape[0], -1)) \n",
    "        first_activation = torch.tanh(merged_feature_vector @ H_weights + H_bias)\n",
    "        second_activation = first_activation @ G_weights + G_bias\n",
    "        \n",
    "\n",
    "        normalized_probabilities = F.log_softmax(second_activation, dim=1)\n",
    "        negative_likelihood_loss = F.nll_loss(normalized_probabilities, Y_dataset)\n",
    "\n",
    "        return negative_likelihood_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "context = 3\n",
    "Xtr, Ytr = build_dataset(words[:n1],context)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2],context)\n",
    "Xte, Yte = build_dataset(words[n2:],context)\n",
    "\n",
    "\n",
    "# META_VARIABLES\n",
    "Context_length=context\n",
    "H_weights_length = 400\n",
    "Feature_dimensions = 10\n",
    "\n",
    "\n",
    "# Parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "feature_vector_C = torch.rand((27, Feature_dimensions), generator=g)\n",
    "H_weights = torch.rand((Context_length*Feature_dimensions, H_weights_length),generator=g)\n",
    "H_bias = torch.rand((H_weights_length) ,generator=g)\n",
    "G_weights = torch.rand((H_weights_length, 27) ,generator=g)\n",
    "G_bias = torch.rand((27,), generator=g)\n",
    "\n",
    "parameters = [feature_vector_C, H_weights, H_bias, G_weights, G_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "lowest_loss = 0\n",
    "for i in range(200000):\n",
    "# minibatch contruct\n",
    "    minibatch_ints = torch.randint(0, Xtr.shape[0], (32,))\n",
    "    minibatch_construct = Xtr[minibatch_ints]\n",
    "# forward pass\n",
    "    feature_activation_layer = feature_vector_C[minibatch_construct] \n",
    "    merged_feature_vector = feature_activation_layer.view((minibatch_construct.shape[0], -1)) \n",
    "    first_activation = torch.tanh(merged_feature_vector @ H_weights + H_bias)\n",
    "    second_activation = first_activation @ G_weights + G_bias   \n",
    "    \n",
    "# loss\n",
    "    normalized_probabilities = F.log_softmax(second_activation, dim=1)\n",
    "    negative_loss_likelihood = F.nll_loss(normalized_probabilities, Ytr[minibatch_ints])\n",
    "# backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    negative_loss_likelihood.backward()\n",
    "\n",
    "#gradient descent\n",
    "\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    loss_item = negative_loss_likelihood.item()\n",
    "    if i == 1:\n",
    "        lowest_loss = loss_item\n",
    "    if lowest_loss > loss_item:\n",
    "        lowest_loss = loss_item\n",
    "\n",
    "    if i % 20000== 0:\n",
    "        print(loss_item)\n",
    "\n",
    "\n",
    "print(lowest_loss, 'lowest loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1146) testing loss on validation set\n",
      "tensor(2.0246) testing loss on training set\n",
      "tensor(2.0999) testing loss on testing set\n"
     ]
    }
   ],
   "source": [
    "testing = test_forward_pass(Xdev, Ydev, feature_vector_C, H_weights, H_bias, G_weights, G_bias)\n",
    "print(testing, ' loss on validation set')\n",
    "testing = test_forward_pass(Xtr, Ytr, feature_vector_C, H_weights, H_bias, G_weights, G_bias)\n",
    "print(testing, ' loss on training set')\n",
    "testing = test_forward_pass(Xte, Yte, feature_vector_C, H_weights, H_bias, G_weights, G_bias)\n",
    "print(testing, ' loss on testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexie.\n",
      "moul.\n",
      "mikion.\n",
      "kayden.\n",
      "maimitta.\n",
      "nella.\n",
      "kaman.\n",
      "arreliyah.\n",
      "javer.\n",
      "gothi.\n",
      "molie.\n",
      "caius.\n",
      "with.\n",
      "arette.\n",
      "kamside.\n",
      "eniavnen.\n",
      "ryonstohlynn.\n",
      "broah.\n",
      "ash.\n",
      "dedri.\n"
     ]
    }
   ],
   "source": [
    "# 300, 3, 200k, feature_dimension=10\n",
    "# tensor(2.1075) testing loss on validation set\n",
    "# tensor(2.0496) testing loss on training set\n",
    "# tensor(2.1113) testing loss on testing set\n",
    "\n",
    "# 400, 3, 300k, feature_dimension=10\n",
    "# tensor(2.1092) testing loss on validation set\n",
    "# tensor(2.0359) testing loss on training set\n",
    "# tensor(2.1201) testing loss on testing set\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * 3 # initialize with all ...\n",
    "    while True:\n",
    "      emb = feature_vector_C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ H_weights + H_bias)\n",
    "      logits = h @ G_weights + G_bias\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
